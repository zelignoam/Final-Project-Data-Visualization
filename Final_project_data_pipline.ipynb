{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8e95fc8",
   "metadata": {},
   "source": [
    "## Visualizatoin final project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c88093",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7272e9b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/noamzelig/Desktop/Noam/Visualization_course/final_project'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "# Set the style for the plots\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.family'] = 'sans-serif' \n",
    "\n",
    "\n",
    "os.getcwd()\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "BASE_DIR = os.path.join(BASE_DIR, 'final_project')\n",
    "BASE_DIR\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe987d00",
   "metadata": {},
   "source": [
    "### load crime_df dataframe from yearly police reports published on gov.il data store using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba00888c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Loaded aggregated crime data from 'crime_raw_cleaned.csv' with shape: (571740, 12)\n",
      "Columns in crime_df_agg: ['Year', 'Quarter', 'QuarterYear', 'Yeshuv', 'YeshuvKod', 'PoliceDistrict', 'PoliceMerhav', 'municipalName', 'StatisticArea', 'StatisticGroup', 'StatisticType', 'EventCount']\n",
      "✔ Crime counts by year:\n",
      "   Year  EventCount\n",
      "0  2020      391532\n",
      "1  2021      208697\n",
      "2  2022      414130\n",
      "3  2023      200342\n",
      "4  2024      197531\n",
      "5  2025      156033\n",
      "Columns in crime_df: ['_id', 'FictiveIDNumber', 'Year', 'Quarter', 'YeshuvKod', 'Yeshuv', 'PoliceDistrictKod', 'PoliceDistrict', 'PoliceMerhavKod', 'PoliceMerhav', 'PoliceStationKod', 'PoliceStation', 'municipalKod', 'municipalName', 'StatisticAreaKod', 'StatisticArea', 'StatisticGroupKod', 'StatisticGroup', 'StatisticTypeKod', 'StatisticType']\n",
      "✔ Crime counts by year:\n",
      "   Year     _id\n",
      "0  2020  391532\n",
      "1  2021  208697\n",
      "2  2022  414130\n",
      "3  2023  200342\n",
      "4  2024  197531\n",
      "5  2025  156033\n"
     ]
    }
   ],
   "source": [
    "crime_df_agg = None\n",
    "crime_df = None\n",
    "crime_df_agg = pd.read_csv('crime_raw_cleaned.csv') # Example filename\n",
    "print(\"✔ Loaded aggregated crime data from 'crime_raw_cleaned.csv' with shape:\", crime_df_agg.shape)\n",
    "print(\"Columns in crime_df_agg:\", crime_df_agg.columns.tolist())    \n",
    "print(\"✔ Crime counts by year:\")   \n",
    "print(crime_df_agg.groupby('Year').agg({'EventCount': 'sum'}).reset_index())\n",
    "\n",
    "crime_df= pd.read_csv('crime_raw.csv') # Example filename\n",
    "print(\"Columns in crime_df:\", crime_df.columns.tolist())\n",
    "\n",
    "print(\"✔ Crime counts by year:\")   \n",
    "print(crime_df.groupby('Year').agg({'_id': 'size'}).reset_index())\n",
    "\n",
    "\n",
    "if crime_df_agg is None:\n",
    "    crime_df = load_crime_df()     \n",
    "    crime_df.to_csv('crime_raw.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"✔ Saved raw crime data to 'crime_raw.csv'\")\n",
    "    print(\"✔ Crime counts by year:\")\n",
    "    print(crime_df.groupby('Year').agg({'_id': 'size'}).reset_index())\n",
    "    \n",
    "\n",
    "\n",
    "def load_crime_df():\n",
    "    # resource_ids by year from data.gov.il\n",
    "    resources = {\n",
    "        \"2025\": \"e311b6a1-be5a-4a82-8298-f3afbee07b6b\",\n",
    "        \"2024\": \"5fc13c50-b6f3-4712-b831-a75e0f91a17e\",\n",
    "        \"2023\": \"32aacfc9-3524-4fba-a282-3af052380244\",\n",
    "        \"2022\": \"a59f3e9e-a7fe-4375-97d0-76cea68382c1\",\n",
    "        \"2021\": \"3f71fd16-25b8-4cfe-8661-e6199db3eb12\",\n",
    "        \"2020\": \"520597e3-6003-4247-9634-0ae85434b971\"\n",
    "    }\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for year, resource_id in resources.items():\n",
    "        print(f\"loading {year}...\")\n",
    "        \n",
    "        # API CKAN – limit 50000, work with offset \n",
    "        url = f\"https://data.gov.il/api/3/action/datastore_search\"\n",
    "        limit = 50000\n",
    "        offset = 0\n",
    "        rows = []\n",
    "        \n",
    "        while True:\n",
    "            resp = requests.get(url, params={\n",
    "                \"resource_id\": resource_id,\n",
    "                \"limit\": limit,\n",
    "                \"offset\": offset\n",
    "            })\n",
    "            data = resp.json()\n",
    "            \n",
    "            batch = data[\"result\"][\"records\"]\n",
    "            rows.extend(batch)\n",
    "            \n",
    "            if len(batch) < limit:\n",
    "                break  # הגיעו לסוף\n",
    "            offset += limit\n",
    "        \n",
    "        df = pd.DataFrame(rows)\n",
    "        all_dfs.append(df)\n",
    "\n",
    "    # Combine all years into a single DataFrame\n",
    "    crime_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    crime_df = crime_df.drop_duplicates()\n",
    "\n",
    "    print(\"✔ Loaded total:\", crime_df.shape[0], \"rows\")\n",
    "    print(\"Columns in crime_df:\", crime_df.columns.tolist())\n",
    "   \n",
    "    #print(crime_df.groupby('Year').size().reset_index(name='EventCount'))   \n",
    "\n",
    "    return crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda6969e",
   "metadata": {},
   "source": [
    "### crime df preproccesing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d47037d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_dataframe_columns(df, columns_to_clean):\n",
    "    \"\"\"\n",
    "    Cleans specified columns in a DataFrame by removing special characters\n",
    "    in a language-agnostic way.\n",
    "    \"\"\"\n",
    "    df_cleaned = df.copy()\n",
    "    pattern = r'[^\\w\\s]|_'\n",
    "    \n",
    "    print(f\"--- Cleaning Text Columns: {columns_to_clean} ---\")\n",
    "    \n",
    "    for col in columns_to_clean:\n",
    "        if col in df_cleaned.columns:\n",
    "            df_cleaned[col] = df_cleaned[col].apply(\n",
    "                lambda x: re.sub(pattern, '', str(x)).strip() if pd.notna(x) else x\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found.\")\n",
    "    print(\"Done.\\n\")\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "def impute_missing_names_from_codes(df, code_name_pairs):\n",
    "    \"\"\"\n",
    "    Imputes missing values in Name columns using their corresponding Code columns.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    print(\"--- Code-Based Imputation Report ---\")\n",
    "    \n",
    "    for code_col, name_col in code_name_pairs:\n",
    "        if code_col not in df_out.columns or name_col not in df_out.columns:\n",
    "            continue\n",
    "            \n",
    "        missing_before = df_out[name_col].isna().sum()\n",
    "        if missing_before == 0: continue\n",
    "            \n",
    "        valid_rows = df_out.dropna(subset=[code_col, name_col])\n",
    "        if valid_rows.empty:\n",
    "            print(f\"Column '{name_col}': No valid reference data found.\")\n",
    "            continue\n",
    "\n",
    "        mapping_df = valid_rows[[code_col, name_col]].drop_duplicates(subset=[code_col])\n",
    "        mapping_dict = dict(zip(mapping_df[code_col], mapping_df[name_col]))\n",
    "        \n",
    "        mask = df_out[name_col].isna() & df_out[code_col].notna()\n",
    "        df_out.loc[mask, name_col] = df_out.loc[mask, code_col].map(mapping_dict)\n",
    "        \n",
    "        missing_after = df_out[name_col].isna().sum()\n",
    "        print(f\"Column '{name_col}': Imputed {missing_before - missing_after} values using '{code_col}'. Remaining: {missing_after}\")\n",
    "\n",
    "    print(\"-------------------------------------\\n\")\n",
    "    return df_out\n",
    "\n",
    "def impute_fields_from_station_text(df, station_col, target_cols):\n",
    "    \"\"\"\n",
    "    Imputes specified target columns (Yeshuv, Merhav, District, etc.) by looking \n",
    "    for their known values inside the PoliceStation text.\n",
    "    \n",
    "    STRICT RULES:\n",
    "    1. Ignores '' (empty strings) and 'מקום אחר' (Other) as valid source values.\n",
    "    2. Prioritizes longer names (greedy match) to avoid partial substring errors.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    if station_col not in df_out.columns:\n",
    "        return df_out\n",
    "        \n",
    "    print(f\"--- Text-Based Imputation (Mining '{station_col}') ---\")\n",
    "    \n",
    "    # Define strictly forbidden values for imputation sources\n",
    "    FORBIDDEN_VALUES = {'', ' ', 'מקום אחר', 'other', 'Other', 'לא ידוע', 'nan', 'None'}\n",
    "\n",
    "    for target_col in target_cols:\n",
    "        if target_col not in df_out.columns:\n",
    "            continue\n",
    "            \n",
    "        missing_before = df_out[target_col].isna().sum()\n",
    "        if missing_before == 0:\n",
    "            print(f\"  - Column '{target_col}': No missing values. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # 1. Get List of known Valid Values\n",
    "        known_values = df_out[target_col].dropna().unique()\n",
    "        \n",
    "        # Filter out forbidden values\n",
    "        valid_candidates = [\n",
    "            v for v in known_values \n",
    "            if str(v).strip() not in FORBIDDEN_VALUES\n",
    "        ]\n",
    "        \n",
    "        # Sort by length descending to match longest possible name first \n",
    "        # (e.g., match \"Tel Aviv-Yafo\" before \"Tel Aviv\")\n",
    "        valid_candidates = sorted(valid_candidates, key=lambda x: len(str(x)), reverse=True)\n",
    "        \n",
    "        if not valid_candidates:\n",
    "            print(f\"  - Column '{target_col}': No valid candidates found for text mining.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Find rows with missing Target but present Station\n",
    "        mask = df_out[target_col].isna() & df_out[station_col].notna()\n",
    "        stations_to_check = df_out.loc[mask, station_col].unique()\n",
    "        \n",
    "        mapping = {}\n",
    "        \n",
    "        for station in stations_to_check:\n",
    "            st_str = str(station)\n",
    "            for candidate in valid_candidates:\n",
    "                cand_str = str(candidate)\n",
    "                # Check if the candidate name is inside the station string\n",
    "                if cand_str in st_str:\n",
    "                    mapping[station] = candidate\n",
    "                    break # Stop after finding the longest match\n",
    "        \n",
    "        # 3. Apply\n",
    "        if not mapping:\n",
    "            print(f\"  - Column '{target_col}': No text matches found.\")\n",
    "        else:\n",
    "            rows_to_fill = mask & df_out[station_col].isin(mapping.keys())\n",
    "            df_out.loc[rows_to_fill, target_col] = df_out.loc[rows_to_fill, station_col].map(mapping)\n",
    "            \n",
    "            filled_count = df_out.loc[mask, target_col].notna().sum()\n",
    "            print(f\"  - Column '{target_col}': Imputed {filled_count} values from Station text.\")\n",
    "\n",
    "    print(\"-----------------------------------------------------------\\n\")\n",
    "    return df_out\n",
    "\n",
    "def impute_parent_from_child(df, child_col, parent_col):\n",
    "    \"\"\"\n",
    "    Imputes missing values in a 'Parent' column (higher hierarchy) based on \n",
    "    values in a 'Child' column (lower hierarchy).\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    if child_col not in df_out.columns or parent_col not in df_out.columns:\n",
    "        return df_out\n",
    "\n",
    "    missing_before = df_out[parent_col].isna().sum()\n",
    "    if missing_before == 0:\n",
    "        return df_out\n",
    "\n",
    "    # 1. Build Mapping\n",
    "    valid_relations = df_out.dropna(subset=[child_col, parent_col])[[child_col, parent_col]].drop_duplicates()\n",
    "    \n",
    "    # Filter out empty/invalid parents from mapping\n",
    "    valid_relations = valid_relations[\n",
    "        ~valid_relations[parent_col].astype(str).isin(['', ' ', 'מקום אחר', 'nan'])\n",
    "    ]\n",
    "\n",
    "    # 2. Check for Ambiguity\n",
    "    ambiguous_children = valid_relations[valid_relations.duplicated(subset=[child_col], keep=False)][child_col].unique()\n",
    "    \n",
    "    if len(ambiguous_children) > 0:\n",
    "        valid_relations = valid_relations[~valid_relations[child_col].isin(ambiguous_children)]\n",
    "    \n",
    "    mapping_dict = dict(zip(valid_relations[child_col], valid_relations[parent_col]))\n",
    "    \n",
    "    # 3. Apply Imputation\n",
    "    mask = df_out[parent_col].isna() & df_out[child_col].notna()\n",
    "    children_triggering_impute = df_out.loc[mask, child_col].unique()\n",
    "    used_map = {k: v for k, v in mapping_dict.items() if k in children_triggering_impute}\n",
    "    \n",
    "    df_out.loc[mask, parent_col] = df_out.loc[mask, child_col].map(mapping_dict)\n",
    "    actual_filled = df_out.loc[mask, parent_col].notna().sum()\n",
    "    \n",
    "    print(f\"Hierarchy Imputation ('{child_col}' -> '{parent_col}'):\")\n",
    "    print(f\"  - Missing Before: {missing_before}\")\n",
    "    print(f\"  - Imputed: {actual_filled}\")\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_manual_code_mapping():\n",
    "    \"\"\"\n",
    "    Returns a dictionary of manual code assignments for cities \n",
    "    where the code might be missing or the name mismatch prevents automatic coding.\n",
    "    Key: City Name (in Crime Data)\n",
    "    Value: Yeshuv Code (Standard CBS Code)\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"נהריה\": 9100,\n",
    "        \"קרית גת\": 2630,\n",
    "        \"גסר א זרקא\": 541,\n",
    "        \"נמל תעופה בן גוריון\": 1748,\n",
    "    }\n",
    "\n",
    "def inject_manual_codes(df, city_col='Yeshuv', code_col='YeshuvCode'):\n",
    "    \"\"\"\n",
    "    Manually injects Yeshuv Codes for specific city names.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The crime dataframe.\n",
    "        city_col (str): The column containing city names.\n",
    "        code_col (str): The column to store/overwrite the City Code.\n",
    "    \"\"\"\n",
    "    mapping = get_manual_code_mapping()\n",
    "    print(f\"--- Injecting Manual Yeshuv Codes into '{code_col}' ---\")\n",
    "\n",
    "    # Ensure code column exists; if not, create it\n",
    "    if code_col not in df.columns:\n",
    "        print(f\"Creating missing column: {code_col}\")\n",
    "        df[code_col] = np.nan\n",
    "\n",
    "    count_updates = 0\n",
    "    # Create a copy to avoid SettingWithCopyWarning\n",
    "    df = df.copy()\n",
    "\n",
    "    for city_name, code in mapping.items():\n",
    "        # Mask: Rows with this city name\n",
    "        mask = (df[city_col] == city_name)\n",
    "        rows_affected = mask.sum()\n",
    "        \n",
    "        if rows_affected > 0:\n",
    "            # We force the code for these rows\n",
    "            df.loc[mask, code_col] = code\n",
    "            print(f\"Set code {code} for '{city_name}': {rows_affected} rows updated.\")\n",
    "            count_updates += rows_affected\n",
    "            \n",
    "    print(f\"✔ Total manual code updates: {count_updates}\\n\")\n",
    "    return df\n",
    "\n",
    "def process_and_summarize_crime_data(df):\n",
    "    \"\"\"\n",
    "    Main execution function to clean, impute (Code, Hierarchy, Text), and aggregate crime data.\n",
    "    \"\"\"\n",
    "    # 0. Print Initial Counts\n",
    "    print(\"--- Initial Data Counts (Before Processing) ---\")\n",
    "    print(f\"Total Records: {len(df)}\")\n",
    "    if 'Year' in df.columns:\n",
    "        print(\"Records by Year:\")\n",
    "        print(df['Year'].value_counts().sort_index())\n",
    "    print(\"-------------------------------------------\\n\")\n",
    "\n",
    "    impute_pairs = [\n",
    "        ('municipalKod', 'municipalName'),\n",
    "        ('YeshuvKod', 'Yeshuv'),\n",
    "        ('PoliceDistrictKod', 'PoliceDistrict'),\n",
    "        ('PoliceMerhavKod', 'PoliceMerhav'),\n",
    "        ('StatisticAreaKod', 'StatisticArea'),\n",
    "        ('StatisticGroupKod', 'StatisticGroup'),\n",
    "        ('StatisticTypeKod', 'StatisticType')\n",
    "    ]\n",
    "    \n",
    "    # 1. Clean Text Columns\n",
    "    text_cols = [pair[1] for pair in impute_pairs] + ['PoliceStation'] \n",
    "    text_cols = [c for c in text_cols if c in df.columns]\n",
    "    df_processed = clean_dataframe_columns(df, text_cols)\n",
    "    #df_processed = inject_manual_codes(df_processed)\n",
    "\n",
    "    \n",
    "    # 2. Impute Names from Codes\n",
    "    df_processed = impute_missing_names_from_codes(df_processed, impute_pairs)\n",
    "    \n",
    "    # 3. Impute Hierarchy (Bottom-Up)\n",
    "    print(\"--- Hierarchical Imputation Sequence ---\")\n",
    "    \n",
    "    # A. Strict Hierarchy: PoliceStation -> Yeshuv\n",
    "    df_processed = impute_parent_from_child(df_processed, 'PoliceStation', 'Yeshuv')\n",
    "    \n",
    "    # B. Text Imputation: PoliceStation -> [Yeshuv, Merhav, District, Muni]\n",
    "    # This checks if the Station Name actually contains the name of the missing field\n",
    "    text_mining_targets = ['Yeshuv', 'PoliceMerhav', 'PoliceDistrict', 'municipalName']\n",
    "    df_processed = impute_fields_from_station_text(df_processed, 'PoliceStation', text_mining_targets)\n",
    "    \n",
    "    # C. Upstream Hierarchy: Yeshuv -> Merhav -> District, Yeshuv -> Muni\n",
    "    # Uses the Yeshuvs/Merhavs filled by steps A and B to propagate further up\n",
    "    remaining_steps = [\n",
    "        ('Yeshuv', 'PoliceMerhav'),        \n",
    "        ('Yeshuv', 'municipalName'),       \n",
    "        ('PoliceMerhav', 'PoliceDistrict') \n",
    "    ]\n",
    "    \n",
    "    for child, parent in remaining_steps:\n",
    "        df_processed = impute_parent_from_child(df_processed, child, parent)\n",
    "    print(\"-------------------------------------------\\n\")\n",
    "\n",
    "    # 4. Add Date Column\n",
    "    df_processed['QuarterYear']= df_processed['Quarter'] +'-'+ df_processed['Year'].astype(str)\n",
    "    #df_processed.head(10000).to_csv(os.path.join(BASE_DIR, 'crime_data_processed.csv'), index=False)\n",
    "    \n",
    "    # 5. Aggregate\n",
    "    group_cols = [\n",
    "        'Year', 'Quarter', 'QuarterYear', 'Yeshuv', 'YeshuvKod',\n",
    "        'PoliceDistrict', 'PoliceMerhav', 'municipalName', \n",
    "        'StatisticArea', 'StatisticGroup', 'StatisticType'\n",
    "    ]\n",
    "    \n",
    "    valid_group_cols = [c for c in group_cols if c in df_processed.columns]\n",
    "    \n",
    "    print(f\"Aggregating by: {valid_group_cols}...\\n\")\n",
    "    \n",
    "    df_for_agg = df_processed.copy()\n",
    "    for col in valid_group_cols:\n",
    "        df_for_agg[col] = df_for_agg[col].fillna('Missing')\n",
    "    \n",
    "    summary_df = df_for_agg.groupby(valid_group_cols)['FictiveIDNumber'].count().reset_index()\n",
    "    summary_df.rename(columns={'FictiveIDNumber': 'EventCount'}, inplace=True)\n",
    "    \n",
    "    print(\"--- Aggregated Summary Stats (After Processing) ---\")\n",
    "    print(f\"Total Events: {summary_df['EventCount'].sum()}\")\n",
    "    print(\"Events by Year (Summary):\")\n",
    "    print(summary_df.groupby('Year')['EventCount'].sum().sort_index())\n",
    "    \n",
    "    return summary_df,df_processed\n",
    "    \n",
    "\n",
    "      \n",
    "crime_df_agg, crime_df_processed = process_and_summarize_crime_data(crime_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab831493",
   "metadata": {},
   "source": [
    "### load population data from gov.il data store using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ee678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# HELPER FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "def clean_dataframe_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Trims whitespace from specified string columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 1. FETCHING DATA (API)\n",
    "# ==========================================\n",
    "\n",
    "def fetch_population_data():\n",
    "    \"\"\"\n",
    "    Fetches population data from data.gov.il API for specific years.\n",
    "    Selects only relevant columns and renames them to English.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Fetch ---\")\n",
    "    \n",
    "    resources = {\n",
    "        2019: '990ae78e-2dae-4a15-a13b-0b5dcc56056c', # Added 2019 for 2020 baseline calc\n",
    "        2020: '2d218594-73e3-40de-b36b-23b22f0a2627',\n",
    "        2021: '95435941-d7e5-46c6-876a-761a74a5928d',\n",
    "        2022: '199b15db-3bcb-470e-ba03-73364737e352',\n",
    "        2023: 'd47a54ff-87f0-44b3-b33a-f284c0c38e5a'\n",
    "    }\n",
    "    \n",
    "    base_url = \"https://data.gov.il/api/3/action/datastore_search\"\n",
    "    pop_dfs = []\n",
    "\n",
    "    # Define the fields to keep and their Hebrew search patterns\n",
    "    field_mapping = {\n",
    "        'Yeshuv_Code': 'סמל יישוב',\n",
    "        'Yeshuv_Name': 'שם יישוב',       \n",
    "        'Religion_Code': 'דת יישוב',\n",
    "        'Total_Population': 'סך הכל אוכלוסייה',\n",
    "        'Total_Israelis': 'סך הכל ישראלים',\n",
    "        'Jews_and_Others': 'יהודים ואחרים',\n",
    "        'Arabs': 'ערבים'\n",
    "    }\n",
    "\n",
    "    for year, resource_id in resources.items():\n",
    "        try:\n",
    "            response = requests.get(base_url, params={'resource_id': resource_id, 'limit': 5000})\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            if data['success']:\n",
    "                records = data['result']['records']\n",
    "                df_temp = pd.DataFrame(records)\n",
    "                \n",
    "                found_cols = {}\n",
    "                \n",
    "                # 1. Yeshuv Name (Exclude 'English')\n",
    "                name_col = next((c for c in df_temp.columns if 'שם יישוב' in c and 'אנגלית' not in c), None)\n",
    "                if name_col:\n",
    "                    found_cols['Yeshuv_Name'] = name_col\n",
    "                \n",
    "                # 2. Find other columns based on keywords\n",
    "                for eng_key, heb_search in field_mapping.items():\n",
    "                    if eng_key == 'Yeshuv_Name': continue \n",
    "                    match = next((c for c in df_temp.columns if heb_search in c), None)\n",
    "                    if match:\n",
    "                        found_cols[eng_key] = match\n",
    "                \n",
    "                if 'Yeshuv_Code' in found_cols or 'Yeshuv_Name' in found_cols:\n",
    "                    rename_map = {v: k for k, v in found_cols.items()}\n",
    "                    df_clean = df_temp[list(found_cols.values())].rename(columns=rename_map).copy()\n",
    "                    df_clean['Year'] = year\n",
    "                    pop_dfs.append(df_clean)\n",
    "                    print(f\"Fetched {year}: {len(df_clean)} rows\")\n",
    "                else:\n",
    "                    print(f\"Warning {year}: Critical columns missing.\")\n",
    "            else:\n",
    "                print(f\"API Error {year}: Success=False\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Connection/Data Error {year}: {e}\")\n",
    "            try:\n",
    "                # Fallback purely for runtime robustness\n",
    "                local_df = pd.read_csv('population_raw.csv')\n",
    "                if year == 2023:\n",
    "                    pop_dfs.append(local_df.rename(columns={\n",
    "                        'סמל יישוב': 'Yeshuv_Code', 'שם יישוב': 'Yeshuv_Name', \n",
    "                        'דת יישוב': 'Religion_Code', 'סך הכל אוכלוסייה 2023 - ארעי': 'Total_Population',\n",
    "                        'שנה': 'Year'\n",
    "                    }))\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    if not pop_dfs:\n",
    "        print(\"No data fetched.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_df = pd.concat(pop_dfs, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "# ==========================================\n",
    "# 2. PREPROCESSING\n",
    "# ==========================================\n",
    "\n",
    "def preprocess_population(df):\n",
    "    print(\"\\n--- Preprocessing Population Data ---\")\n",
    "    if df.empty: return df\n",
    "\n",
    "    df['Yeshuv_Code'] = pd.to_numeric(df['Yeshuv_Code'], errors='coerce')\n",
    "    df = clean_dataframe_columns(df, ['Yeshuv_Name'])\n",
    "    \n",
    "    numeric_cols = ['Total_Population', 'Total_Israelis', 'Jews_and_Others', 'Arabs', 'Religion_Code']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            if df[col].dtype == object:\n",
    "                 df[col] = df[col].astype(str).str.replace(',', '')\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    print(f\"Processed {len(df)} rows. Columns: {df.columns.tolist()}\")\n",
    "    return df\n",
    "\n",
    "# ==========================================\n",
    "# 3. QUARTERLY EXTRAPOLATION\n",
    "# ==========================================\n",
    "\n",
    "def extrapolate_quarters(pop_df, years_of_interest):\n",
    "    print(\"\\n--- Extrapolating Quarters ---\")\n",
    "    expanded_data = []\n",
    "    \n",
    "    valid_pop = pop_df.dropna(subset=['Yeshuv_Code']).copy()\n",
    "    debug_count = 0\n",
    "    \n",
    "    for yeshuv_code, group in valid_pop.groupby('Yeshuv_Code'):\n",
    "        group = group.sort_values('Year')\n",
    "        yeshuv_name = group['Yeshuv_Name'].iloc[0]\n",
    "        \n",
    "        for year in years_of_interest:\n",
    "            current_row = group[group['Year'] == year]\n",
    "            if current_row.empty: continue\n",
    "            \n",
    "            curr_pop = current_row['Total_Population'].values[0]\n",
    "            if pd.isna(curr_pop): curr_pop = 0\n",
    "            \n",
    "            # Optional attributes\n",
    "            rel_code = current_row['Religion_Code'].values[0] if 'Religion_Code' in current_row else np.nan\n",
    "            israelis = current_row['Total_Israelis'].values[0] if 'Total_Israelis' in current_row else np.nan\n",
    "            jews = current_row['Jews_and_Others'].values[0] if 'Jews_and_Others' in current_row else np.nan\n",
    "            arabs = current_row['Arabs'].values[0] if 'Arabs' in current_row else np.nan\n",
    "\n",
    "            # Calculate Growth\n",
    "            prev_row = group[group['Year'] == year - 1]\n",
    "            growth_calculated = False\n",
    "\n",
    "            if not prev_row.empty:\n",
    "                prev_pop = prev_row['Total_Population'].values[0]\n",
    "                if pd.notna(prev_pop):\n",
    "                    diff = curr_pop - prev_pop\n",
    "                    q_growth = diff / 4\n",
    "                    quarters = [prev_pop + q_growth, prev_pop + (q_growth*2), prev_pop + (q_growth*3), curr_pop]\n",
    "                    growth_calculated = True\n",
    "                    \n",
    "            if not growth_calculated:\n",
    "                quarters = [curr_pop] * 4\n",
    "            \n",
    "            for q_idx, q_name in enumerate(['Q1', 'Q2', 'Q3', 'Q4']):\n",
    "                val = quarters[q_idx]\n",
    "                pop_val = int(val) if pd.notna(val) else 0\n",
    "\n",
    "                expanded_data.append({\n",
    "                    'Yeshuv_Code': yeshuv_code,\n",
    "                    'Yeshuv_Name': yeshuv_name,\n",
    "                    'Year': year,\n",
    "                    'Quarter': q_name,\n",
    "                    'Total_Population': pop_val, \n",
    "                    'Religion_Code': rel_code,\n",
    "                    'Total_Israelis': israelis,\n",
    "                    'Jews_and_Others': jews,\n",
    "                    'Arabs': arabs\n",
    "                })\n",
    "                \n",
    "    return pd.DataFrame(expanded_data)\n",
    "\n",
    "# ==========================================\n",
    "# 4. JOIN WITH CRIME DATA\n",
    "# ==========================================\n",
    "\n",
    "def join_crime_population(crime_df_agg, pop_quarterly_df):\n",
    "    \"\"\"\n",
    "    Joins crime DataFrame with the extrapolated population data.\n",
    "    Input:\n",
    "        crime_df_agg: pandas DataFrame containing aggregated crime data.\n",
    "        pop_quarterly_df: pandas DataFrame containing population data.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Starting Join Operation ---\")\n",
    "    \n",
    "    # 1. Clean Crime Data before merge\n",
    "    # Remove rows where crucial keys are missing in the crime data itself\n",
    "    crime_df_clean = crime_df_agg.dropna(subset=['Yeshuv', 'Year']).copy()\n",
    "    \n",
    "    # Prepare Keys\n",
    "    crime_df_clean['Join_Key_Code'] = pd.to_numeric(crime_df_clean['YeshuvKod'], errors='coerce')\n",
    "    crime_df_clean['Join_Key_Name'] = crime_df_clean['Yeshuv'].astype(str).str.strip()\n",
    "    \n",
    "    total_crime_rows = len(crime_df_clean)\n",
    "    \n",
    "    # Columns to merge from population\n",
    "    merge_cols = ['Yeshuv_Code', 'Year', 'Quarter', 'Total_Population', 'Religion_Code', \n",
    "                  'Total_Israelis', 'Jews_and_Others', 'Arabs']\n",
    "    \n",
    "    available_cols = [c for c in merge_cols if c in pop_quarterly_df.columns]\n",
    "    \n",
    "    # 2. Join by Code\n",
    "    print(\"...Matching by Yeshuv Code\")\n",
    "    merged_df = pd.merge(\n",
    "        crime_df_clean,\n",
    "        pop_quarterly_df[available_cols],\n",
    "        left_on=['Join_Key_Code', 'Year', 'Quarter'],\n",
    "        right_on=['Yeshuv_Code', 'Year', 'Quarter'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 3. Join by Name (Fallback)\n",
    "    print(\"...Matching by Yeshuv Name (Fallback)\")\n",
    "    name_merge_cols = ['Yeshuv_Name', 'Year', 'Quarter'] + [c for c in available_cols if c not in ['Yeshuv_Code', 'Year', 'Quarter']]\n",
    "    \n",
    "    merged_with_name = pd.merge(\n",
    "        merged_df,\n",
    "        pop_quarterly_df[name_merge_cols],\n",
    "        left_on=['Join_Key_Name', 'Year', 'Quarter'],\n",
    "        right_on=['Yeshuv_Name', 'Year', 'Quarter'],\n",
    "        how='left',\n",
    "        suffixes=('', '_NameFallback')\n",
    "    )\n",
    "    \n",
    "    # Coalesce values\n",
    "    target_fields = ['Total_Population', 'Religion_Code', 'Total_Israelis', 'Jews_and_Others', 'Arabs']\n",
    "    for col in target_fields:\n",
    "        fallback_col = f'{col}_NameFallback'\n",
    "        if fallback_col in merged_with_name.columns:\n",
    "             merged_with_name[col] = merged_with_name[col].fillna(merged_with_name[fallback_col])\n",
    "    \n",
    "    # Cleanup auxiliary columns\n",
    "    cols_to_drop = [c for c in merged_with_name.columns if 'Join_Key' in c or '_NameFallback' in c]\n",
    "    merged_final = merged_with_name.drop(columns=cols_to_drop, errors='ignore')\n",
    "\n",
    "    # --- POST MERGE (Keeping Missing Population) ---\n",
    "    print(\"...Analyzing match results (Keeping unmatched rows)\")\n",
    "    # Previously we dropped these: merged_final.dropna(subset=['Total_Population'])\n",
    "    # Now we keep them.\n",
    "    total_rows = len(merged_final)\n",
    "    matched_rows = merged_final['Total_Population'].notna().sum()\n",
    "    unmatched_rows = total_rows - matched_rows\n",
    "\n",
    "    # --- COLUMN CONSOLIDATION ---\n",
    "    print(\"...Consolidating duplicate ID columns\")\n",
    "    # 1. Names: Keep 'Yeshuv', fill from 'Yeshuv_Name' if missing, then drop 'Yeshuv_Name'\n",
    "    if 'Yeshuv' in merged_final.columns and 'Yeshuv_Name' in merged_final.columns:\n",
    "        merged_final['Yeshuv'] = merged_final['Yeshuv'].fillna(merged_final['Yeshuv_Name'])\n",
    "        merged_final.drop(columns=['Yeshuv_Name'], inplace=True)\n",
    "\n",
    "    # 2. Codes: Keep 'YeshuvKod', fill from 'Yeshuv_Code' if missing, then drop 'Yeshuv_Code'\n",
    "    if 'YeshuvKod' in merged_final.columns and 'Yeshuv_Code' in merged_final.columns:\n",
    "        merged_final['YeshuvKod'] = merged_final['YeshuvKod'].fillna(merged_final['Yeshuv_Code'])\n",
    "        merged_final.drop(columns=['Yeshuv_Code'], inplace=True)\n",
    "\n",
    "    # Stats\n",
    "    print(\"\\n=== Merge Stats ===\")\n",
    "    print(f\"Total Output Rows: {total_rows}\")\n",
    "    print(f\"Rows with Population Data: {matched_rows}\")\n",
    "    print(f\"Rows missing Population Data: {unmatched_rows}\")\n",
    "    print(\"===================\\n\")\n",
    "\n",
    "    return merged_final\n",
    "\n",
    "# ==========================================\n",
    "# MAIN EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    final_filename = 'merged_crime_population_final.csv'\n",
    "    \n",
    "    # --- CHECK FOR EXISTING FINAL FILE ---\n",
    "    if os.path.exists(final_filename):\n",
    "        print(f\"✔ Found existing final dataset: '{final_filename}'. Loading directly...\")\n",
    "        final_df = pd.read_csv(final_filename)\n",
    "        \n",
    "        print(\"--- Dataset Statistics ---\")\n",
    "        print(final_df.info())\n",
    "        \n",
    "        # Reset pandas display format\n",
    "        pd.reset_option('display.float_format')\n",
    "\n",
    "        print(\"\\nSample Data:\")\n",
    "        print(final_df[['Year', 'Yeshuv', 'Total_Population']].head())\n",
    "        \n",
    "    else:\n",
    "        # --- 1. LOAD & CLEAN CRIME DATA ---\n",
    "        print(\"1. Loading and Cleaning Crime Data...\")\n",
    "        # NOTE: In a real run, load your csv here. \n",
    "        # For this script to work standalone, we assume 'crime_df_agg' might exist in memory or load it.\n",
    "        try:\n",
    "            # Try loading from file if variable doesn't exist (simulated environment)\n",
    "            if 'crime_df_agg' not in globals():\n",
    "                crime_df_agg = pd.read_csv('crime_data_processed.csv') # Example filename\n",
    "                \n",
    "            initial_crime_count = len(crime_df_agg)\n",
    "            \n",
    "            # Step 1: Remove Duplicates\n",
    "            crime_df_agg = crime_df_agg.drop_duplicates()\n",
    "            print(f\"   - Removed {initial_crime_count - len(crime_df_agg)} duplicates.\")\n",
    "            \n",
    "            # Step 2: Save Raw Crime CSV\n",
    "            crime_df_agg.to_csv('crime_raw_cleaned.csv', index=False, encoding='utf-8-sig')\n",
    "            print(\"   - Saved 'crime_raw_cleaned.csv'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading crime data: {e}\")\n",
    "            crime_df_agg = None\n",
    "\n",
    "        # --- 2. PROCESS POPULATION DATA ---\n",
    "        print(\"\\n2. Processing Population Data...\")\n",
    "        pop_processed_filename = 'population_processed.csv'\n",
    "        pop_quarterly = pd.DataFrame()\n",
    "\n",
    "        if os.path.exists(pop_processed_filename):\n",
    "            print(f\"   - Found '{pop_processed_filename}', loading directly...\")\n",
    "            pop_quarterly = pd.read_csv(pop_processed_filename)\n",
    "        else:\n",
    "            pop_raw = fetch_population_data()\n",
    "            \n",
    "            if not pop_raw.empty:\n",
    "                pop_clean = preprocess_population(pop_raw)\n",
    "                \n",
    "                # Extrapolate\n",
    "                pop_quarterly = extrapolate_quarters(pop_clean, years_of_interest=[2020, 2021, 2022, 2023])\n",
    "                \n",
    "                # Step 3: Save Population CSV\n",
    "                pop_quarterly.to_csv(pop_processed_filename, index=False, encoding='utf-8-sig')\n",
    "                print(f\"   - Saved '{pop_processed_filename}'\")\n",
    "\n",
    "        if not pop_quarterly.empty:\n",
    "            # --- 3. MERGE & STATS ---\n",
    "            if crime_df_agg is not None:\n",
    "                print(\"\\n3. Merging Data...\")\n",
    "                \n",
    "                # Step 4 & 5: Merge (now keeping missing)\n",
    "                final_df = join_crime_population(crime_df_agg, pop_quarterly)\n",
    "                \n",
    "                # Step 6: Print Stats and Save\n",
    "                print(\"--- Final Dataset Statistics ---\")\n",
    "                print(final_df.info())\n",
    "                \n",
    "                # Reset pandas display format\n",
    "                pd.reset_option('display.float_format')\n",
    "\n",
    "                print(\"\\nSample Data:\")\n",
    "                print(final_df[['Year', 'Yeshuv', 'Total_Population']].head())\n",
    "                \n",
    "                # Step 7: Save to CSV\n",
    "                print(f\"\\n--- Saving Final Data ---\")\n",
    "                final_df.to_csv(final_filename, index=False, encoding='utf-8-sig')\n",
    "                print(f\"✔ Saved final dataset to: {os.path.abspath(final_filename)}\")\n",
    "        else:\n",
    "            print(\"⚠ Population data unavailable (Fetch failed and no local file).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e48be6",
   "metadata": {},
   "source": [
    "### load cost price index data from halamas data store using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9b1db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chained_quarterly_cpi(start_year, end_year):\n",
    "    # 1. API Parameters\n",
    "    cpi_id = 120010\n",
    "    start_period = f\"01-{start_year}\"\n",
    "    end_period = f\"12-{end_year}\"\n",
    "    \n",
    "    url = \"https://api.cbs.gov.il/index/data/price\"\n",
    "    \n",
    "    params = {\n",
    "        \"id\": cpi_id,\n",
    "        \"startPeriod\": start_period,\n",
    "        \"endPeriod\": end_period,\n",
    "        \"format\": \"json\",\n",
    "        \"download\": \"false\"\n",
    "    }\n",
    "\n",
    "    print(f\"Fetching data from CBS API for {start_year}-{end_year}...\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        # Verify data structure exists\n",
    "        if 'month' not in data or not data['month']:\n",
    "            print(\"No data found.\")\n",
    "            return None\n",
    "\n",
    "        # Navigate to the list of observations\n",
    "        # Structure: data['month'][0] contains the series info\n",
    "        # Inside that, 'date' is the list of monthly records\n",
    "        series_data = data['month'][0] \n",
    "        observations = series_data.get('date', [])\n",
    "        \n",
    "        records = []\n",
    "        for obs in observations:\n",
    "            # Extract date components\n",
    "            year = obs['year']\n",
    "            month = obs['month']\n",
    "            \n",
    "            # Extract monthly percentage change\n",
    "            # Note: The key in the JSON is simply 'percent'\n",
    "            pct_change = obs.get('percent')\n",
    "            \n",
    "            if pct_change is not None:\n",
    "                date_str = f\"{year}-{month:02d}-01\"\n",
    "                records.append({\n",
    "                    'date': date_str,\n",
    "                    'monthly_percent_change': float(pct_change)\n",
    "                })\n",
    "            \n",
    "        df = pd.DataFrame(records)\n",
    "        \n",
    "        # Convert to datetime and sort ascending (Crucial for chaining)\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # 2. Calculate Chained Index (Normalization)\n",
    "        # We start with a base of 100.0 points at the beginning of the period\n",
    "        df['chained_index'] = 100.0\n",
    "        \n",
    "        # Iterate and calculate cumulative index based on percentage changes\n",
    "        # Formula: New_Index = Old_Index * (1 + percent_change / 100)\n",
    "        # We skip the first row (it stays 100.0) and calculate from the second onwards\n",
    "        for i in range(1, len(df)):\n",
    "            prev_index = df.loc[i-1, 'chained_index']\n",
    "            change_pct = df.loc[i, 'monthly_percent_change']\n",
    "            \n",
    "            new_index = prev_index * (1 + change_pct / 100)\n",
    "            df.loc[i, 'chained_index'] = new_index\n",
    "\n",
    "        # 3. Quarterly Aggregation\n",
    "        # We calculate the MEAN of the chained index for the quarter\n",
    "        # We also sum the percentage changes to see \"Quarterly Inflation\"\n",
    "        quarterly_df = df.set_index('date').resample('Q').agg({\n",
    "            'chained_index': 'mean',\n",
    "            'monthly_percent_change': 'sum' # Sum of monthly changes approx equals quarterly inflation\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Formatting\n",
    "        quarterly_df['quarter_name'] = quarterly_df['date'].dt.to_period('Q')\n",
    "        quarterly_df = quarterly_df.rename(columns={\n",
    "            'chained_index': 'avg_chained_index_points',\n",
    "            'monthly_percent_change': 'total_quarterly_inflation_pct'\n",
    "        })\n",
    "        \n",
    "        return quarterly_df[['quarter_name', 'avg_chained_index_points', 'total_quarterly_inflation_pct']]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    cpi_df = get_chained_quarterly_cpi(2020, 2025)\n",
    "    cpi_df.to_csv('quarterly_cpi_chained.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"✔ Saved quarterly CPI data to 'quarterly_cpi_chained.csv'\")\n",
    "    \n",
    "    if cpi_df is not None:\n",
    "        print(\"\\n--- Quarterly CPI (Chained, Base Jan 2020 = 100) ---\")\n",
    "        print(cpi_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
